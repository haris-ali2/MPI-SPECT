\chapter{Related Work}
\label{ch:related}

The automatic reorientation and segmntation  process of MPI SPECT represent steps that are essential for the accurate and and efficient diagnosis andd the quantitative analysis of the heart. A number of commercial softwares have been developed in order to perform these tasks and are widely adopted in clinical practices. The Corridor4DM \cite{FICARO2007455}, which was developed at the university of Michigan ,provides a platform for the comprehensive quantitative analysis for Myocardial Perfusion and the functional assesment from SPECT. This extremely integrated system gives access to an automated processing tool for analysis and reporting  which si specifically developed to meet the increasing demands of such tools. In a similar way, the Emory Cardiac Toolbox (ECTb) \cite{GARCIA2007420} implements an extensive pipeline of quantitative tools which are developed as a result of very extensive research and its validation. It features a database of normal perfusion with more than 150 patients each, the fourier analysis of regional thikening, used for functional assesment, and a very advanced display function that allows to display 3D volumes for image fusion. The Cedars-Sinai approach \cite{GERMANO2007433} fouses on an end-to-end automatic expert system which is based on mathematical algorithms and rules based on logic reasoning. The presented QGS software is been used at more than 20 thousand locations all across the globe. The yale method \cite{LIU2007483} is focused on the quantification process of both the MPI and specifically the LV funcctional abnormalities, which adress the challenges faced by the process by muultiple factors such as background of images and the defects of perfusion  using specialized processing techniques.

Other than the mentioned commercial solutions for the tasks, there is significant amount of research efforts that have been devotedly carried out to develop more advanced approaches and algorithms for MPI SPECT segmentation and reorientation. The Level-Set Methods (LSMs) have been proved to me one of the most developed methods in the field. The research by \cite{DBLP:journals/cars/HosntalabMMA12} presented an automatic method for the segmentation of the LV based on variational level sets in volumetri SPECT. This methos integrates adaptive thresholding for initialization with the evolution of variational level set for the determination of the finall contour. Very efective performance has been demonstrated using this approach as compared to the manual delineation through ROC analysis. More advanced LSM techniques \cite{10.1007/11866565_12} developeda model for implicit level sets representations which is based on 4D statistical shape analysis that combined the temporal information  gotten from gated SPECT sequences. This eliminatedd the need for challenging point correspondences, at the same time outperforming 3D models with a better characterization of the evolution of the temporal shape.

Multiple hybrid approaches have also been developed in order to address some specific challenges in MPI SPECT analysis. The charged contour model presented in \cite{Yang2006ACC} is designed specifically to handlle the concavities in the segmentation label volumes. Later, \cite{https://doi.org/10.1049/iet-cvi.2012.0081} proposed a novel approach that combined the shape and appearance priors using a constraint with levels set deformable models, hence implementing a soft-to-hard parobabiitic constraint that provided a lot more flexibility as compared to rigid shape constraints alone. This approach proved to be particularly efective for LV segmentation in 4 dimensional gated SPECT, even if there were perfusion defects present. \cite{8409947} developed  hybrid active contour model for Myocardial D-SPECT volumes thar combined local image fitting models with the region-scalable fitting energy functions in order to mitigate the inhomogeneity issues, all the while maintaining the computational effciency. More earlier work by \cite{7123558} developed a statistical model-based approach with the usage of 3D Active Shape Models (ASM) which combined both the geometric shape and the information depicted by the grey-level appearance from training data for the purpose of achieving robust segmentation of gated SPECT MPI.

Despite all the advances in the research, the traditional approaches stil continue to face great challenges in order to achieve the globally optimal point especially when processing the complete field-of-view volumes with a varying amount of image quality and variability in the anatomy. Such limitations has drived the more recent exploration of the machine and deep learning techniques in the field of nuclear cardiology. Early machine larning applications in the domain were specifically focused on sub-tasks of the whole segmentation pipeline. \cite{Betancur2017} showed the effectiveness of using support vector machines (SVMs) for predicting the optimal valve positioning, demonstraing that the approach can be comparable to expert performance in SPECT alignment, at thesame time reducing the dependance on users for quantification. This study highlighted how even the conventional machine learning approaches can improve some specific aspects of the workflow of cardiac analysis. The study done in \cite{ARABI2021122} presents a comprehensive review showing that deep learning solutions have shown remarkable promise across multiple aspects of PET and SPECT imaging, from the quantitative analysis to the instrumentation part of it all. \cite{WOLTERINK2020988} presentes a specific discussion about the transformative impace of using convolutional neural networks (CNNs) on the task of LV segmentaion, showing their ability to learn and interpret complex features directly from images. 

The dawn of deep learning in the world brought forward even more comprehensive solutions to the task of LV segmentation. \cite{Wang2020} proposes an end-to-end fully CNN based architecture that directly learns the segmentation mapping from the SPECT images taken as input. This approach eliminated the need to process the volumes in multiple stages for the segmentation map. This way the approaches using deep learning in order to handle the entire segmentation task could be developed, replacing the traditional way, with a unified framework. Buiding on the same idea \cite{WEN2021166842} implemented a U-Net \cite{RFB15a} based CNN architecture that outperformed significantly their own previous dynamic programming solution presented in \cite{tang2017dynamic}, and it particularly improves handling the complex variations of shape of the Lv myocardim. More recently, there have been an increasing number of studies in even more sophisticated network architectures that are tailored to some of the unique challenges that are prevelant in the analaysis of cardiac SPECT. \cite{zhao2023spatial} introduced convolutional long-short term memory (LSTM) units in the skip connections of a V-Net architecture \cite{7785132}, which enabled an effective way of extracting temporal features from gated SPECT sequences. This novel approach addressed an essential need to leverage the presence of temporal information in higher dimensional volumes of SPECT. In a similar way, \cite{ZHANG2023107267} enhances the usual 3D U-Net arhitecture with a self-attention mechanism which is amployed at the bottleneck. This allows for better inclusion of the global contexual information throughout the volumetric data.

The usage of shape priors have been identified as one o the efficient strategies, valuable for improving the accuracy of segmentation models. \cite{ZHU2023106954} proposes a method that includes the shape priors, which are generated using a dynamic programming algorithm into a 3D V-Net network using a spatial transformer network (STN) which proved to give extremely good resuts al the whhile maintaining anatomical consistency. Despite the research, there is still a lot of unexplored room when it comes to shape priors and their use in the segmentation process \cite{Wang2020}. The existing solutions rely heavily on a number of factors. The first one being the availability of a huge dataset size available with labels in order to train a model. Secondly, the focus within the architecture have been convolutional neural networks and thirdly, even the use of hybrid mechanisms which include attension systems have at least one convolutional layer component. While being effective, these approaches most of the time require substantial training data or very complex model architecture that might limit clinical applicability.

As opposed to the previous works in the field, the method proposed in this study introduces a number of innovative ideas. Firstly, the DL model used in the study is a fully transformer based architecture, which diverts from the traditional approaches of using convolutional blocks. Using this approach allows to capture long-range global dependancies in the input 3D volumes. Moreover, and more importantly, our approach incorporates the statistical shape priors in a way that compliments the strength of the transformer model for better performance. This approach achieves performance that is comparable to the state-of-the-art methods, all the while requiring half the amount of training, with limited data, addressing a probllem that is critical in the clinical deployement where there is not a huge amount of annotated dataset available. The novelty of this research is built upon, with substantial etension of the previously present work. Even though \cite{10488032} demonstrates the value of using spatial transformers and  and \cite{ZHU2023106954} discuss te usage of shape priors, this method unifies the approaches within a single transformer framework. Compared to the attention methhod presented in \cite{ZHANG2023107267}, the approach presented in this study is more comprehensive self-attention based paradigm throughout the whole network. The gains in the efficiency, relative to \cite{10.1145/3632047.3632078} are very noteworthy, as a strong performance is achieved her without the need of a self-supervised pretraining phase.